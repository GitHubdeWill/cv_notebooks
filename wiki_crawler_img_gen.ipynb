{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import cv2\n",
    "from matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/4460921/extract-the-first-paragraph-from-a-wikipedia-article-python\n",
    "import re\n",
    "import yaml\n",
    "import urllib\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = {\n",
    "    'en': 'https://en.wikipedia.org',\n",
    "    'zh': 'https://zh.wikipedia.org'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikipedia:\n",
    "    url_article = 'http://%s.wikipedia.org/w/index.php?action=raw&title=%s'\n",
    "    url_image = 'http://%s.wikipedia.org/w/index.php?title=Special:FilePath&file=%s'\n",
    "    url_search = 'http://%s.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&sroffset=%d&srlimit=%d&format=yaml'\n",
    "   \n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "   \n",
    "    def __fetch(self, url):\n",
    "        request = urllib2.Request(url)\n",
    "        request.add_header('User-Agent', 'Mozilla/5.0')\n",
    "       \n",
    "        try:\n",
    "            result = urllib2.urlopen(request)\n",
    "        except urllib2.HTTPError, e:\n",
    "            raise WikipediaError(e.code)\n",
    "        except urllib2.URLError, e:\n",
    "            raise WikipediaError(e.reason)\n",
    "       \n",
    "        return result\n",
    "   \n",
    "    def article(self, article):\n",
    "        url = self.url_article % (self.lang, urllib.quote_plus(article))\n",
    "        content = self.__fetch(url).read()\n",
    "       \n",
    "        if content.upper().startswith('#REDIRECT'):\n",
    "            match = re.match('(?i)#REDIRECT \\[\\[([^\\[\\]]+)\\]\\]', content)\n",
    "           \n",
    "            if not match == None:\n",
    "                return self.article(match.group(1))\n",
    "           \n",
    "            raise WikipediaError('Can\\'t found redirect article.')\n",
    "       \n",
    "        return content\n",
    "   \n",
    "    def image(self, image, thumb=None):\n",
    "        url = self.url_image % (self.lang, image)\n",
    "        result = self.__fetch(url)\n",
    "        content = result.read()\n",
    "       \n",
    "        if thumb:\n",
    "            url = result.geturl() + '/' + thumb + 'px-' + image\n",
    "            url = url.replace('/commons/', '/commons/thumb/')\n",
    "            url = url.replace('/' + self.lang + '/', '/' + self.lang + '/thumb/')\n",
    "           \n",
    "            return self.__fetch(url).read()\n",
    "       \n",
    "        return content\n",
    "   \n",
    "    def search(self, query, page=1, limit=10):\n",
    "        offset = (page - 1) * limit\n",
    "        url = self.url_search % (self.lang, urllib.quote_plus(query), offset, limit)\n",
    "        content = self.__fetch(url).read()\n",
    "       \n",
    "        parsed = yaml.load(content)\n",
    "        search = parsed['query']['search']\n",
    "       \n",
    "        results = []\n",
    "       \n",
    "        if search:\n",
    "            for article in search:\n",
    "                title = article['title'].strip()\n",
    "               \n",
    "                snippet = article['snippet']\n",
    "                snippet = re.sub(r'(?m)<.*?>', '', snippet)\n",
    "                snippet = re.sub(r'\\s+', ' ', snippet)\n",
    "                snippet = snippet.replace(' . ', '. ')\n",
    "                snippet = snippet.replace(' , ', ', ')\n",
    "                snippet = snippet.strip()\n",
    "               \n",
    "                wordcount = article['wordcount']\n",
    "               \n",
    "                results.append({\n",
    "                    'title' : title,\n",
    "                    'snippet' : snippet,\n",
    "                    'wordcount' : wordcount\n",
    "                })\n",
    "       \n",
    "        # yaml.dump(results, default_style='', default_flow_style=False,\n",
    "        #     allow_unicode=True)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki2Plain:\n",
    "    def __init__(self, wiki):\n",
    "        self.wiki = wiki\n",
    "       \n",
    "        self.text = wiki\n",
    "        self.text = self.unhtml(self.text)\n",
    "        self.text = self.unwiki(self.text)\n",
    "        self.text = self.punctuate(self.text)\n",
    "   \n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "   \n",
    "    def unwiki(self, wiki):\n",
    "        \"\"\"\n",
    "       Remove wiki markup from the text.\n",
    "       \"\"\"\n",
    "        wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "        wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
    "        wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
    "        wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
    "        wiki = re.sub(r\"''+\", '', wiki)\n",
    "        wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
    "       \n",
    "        return wiki\n",
    "   \n",
    "    def unhtml(self, html):\n",
    "        \"\"\"\n",
    "       Remove HTML from the text.\n",
    "       \"\"\"\n",
    "        html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
    "        html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
    "        html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
    "        html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
    "        html = re.sub(r'(?m)<.*?>', '', html)\n",
    "        html = re.sub(r'(?i)&amp;', '&', html)\n",
    "       \n",
    "        return html\n",
    "   \n",
    "    def punctuate(self, text):\n",
    "        \"\"\"\n",
    "       Convert every text part into well-formed one-space\n",
    "       separate paragraph.\n",
    "       \"\"\"\n",
    "        text = re.sub(r'\\r\\n|\\n|\\r', '\\n', text)\n",
    "        text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "       \n",
    "        parts = text.split('\\n\\n')\n",
    "        partsParsed = []\n",
    "       \n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "           \n",
    "            if len(part) == 0:\n",
    "                continue\n",
    "           \n",
    "            partsParsed.append(part)\n",
    "       \n",
    "        return '\\n\\n'.join(partsParsed)\n",
    "   \n",
    "    def image(self):\n",
    "        \"\"\"\n",
    "       Retrieve the first image in the document.\n",
    "       \"\"\"\n",
    "        # match = re.search(r'(?i)\\|?\\s*(image|img|image_flag)\\s*=\\s*(<!--.*-->)?\\s*([^\\\\/:*?<>\"|%]+\\.[^\\\\/:*?<>\"|%]{3,4})', self.wiki)\n",
    "        match = re.search(r'(?i)([^\\\\/:*?<>\"|% =]+)\\.(gif|jpg|jpeg|png|bmp)', self.wiki)\n",
    "       \n",
    "        if match:\n",
    "            return '%s.%s' % match.groups()\n",
    "       \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwiki(wiki):\n",
    "    \"\"\"\n",
    "   Remove wiki markup from the text.\n",
    "   \"\"\"\n",
    "    wiki = re.sub(r'(?i)\\{\\{IPA(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "    wiki = re.sub(r'(?i)\\{\\{Lang(\\-[^\\|\\{\\}]+)*?\\|([^\\|\\{\\}]+)(\\|[^\\{\\}]+)*?\\}\\}', lambda m: m.group(2), wiki)\n",
    "    wiki = re.sub(r'\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "    wiki = re.sub(r'(?m)\\{\\{[^\\{\\}]+\\}\\}', '', wiki)\n",
    "    wiki = re.sub(r'(?m)\\{\\|[^\\{\\}]*?\\|\\}', '', wiki)\n",
    "    wiki = re.sub(r'(?i)\\[\\[Category:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "    wiki = re.sub(r'(?i)\\[\\[Image:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "    wiki = re.sub(r'(?i)\\[\\[File:[^\\[\\]]*?\\]\\]', '', wiki)\n",
    "    wiki = re.sub(r'\\[\\[[^\\[\\]]*?\\|([^\\[\\]]*?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "    wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', lambda m: m.group(1), wiki)\n",
    "    wiki = re.sub(r'\\[\\[([^\\[\\]]+?)\\]\\]', '', wiki)\n",
    "    wiki = re.sub(r'(?i)File:[^\\[\\]]*?', '', wiki)\n",
    "    wiki = re.sub(r'\\[[^\\[\\]]*? ([^\\[\\]]*?)\\]', lambda m: m.group(1), wiki)\n",
    "    wiki = re.sub(r\"''+\", '', wiki)\n",
    "    wiki = re.sub(r'(?m)^\\*$', '', wiki)\n",
    "\n",
    "    return wiki\n",
    "\n",
    "def unhtml(html):\n",
    "    \"\"\"\n",
    "   Remove HTML from the text.\n",
    "   \"\"\"\n",
    "    html = re.sub(r'(?i)&nbsp;', ' ', html)\n",
    "    html = re.sub(r'(?i)<br[ \\\\]*?>', '\\n', html)\n",
    "    html = re.sub(r'(?m)<!--.*?--\\s*>', '', html)\n",
    "    html = re.sub(r'(?i)<ref[^>]*>[^>]*<\\/ ?ref>', '', html)\n",
    "    html = re.sub(r'(?m)<.*?>', '', html)\n",
    "    html = re.sub(r'(?i)&amp;', '&', html)\n",
    "\n",
    "    return html\n",
    "\n",
    "def punctuate(text):\n",
    "    \"\"\"\n",
    "   Convert every text part into well-formed one-space\n",
    "   separate paragraph.\n",
    "   \"\"\"\n",
    "    text = re.sub(r'\\r\\n|\\n|\\r', '\\n', text)\n",
    "    text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "\n",
    "    parts = text.split('\\n\\n')\n",
    "    partsParsed = []\n",
    "\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "\n",
    "        if len(part) == 0:\n",
    "            continue\n",
    "\n",
    "        partsParsed.append(part)\n",
    "\n",
    "    return '\\n\\n'.join(partsParsed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = urllib2.urlopen('https://en.wikipedia.org/wiki/Portal:Contents/Lists')\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "base=soup.find('div', id=\"bodyContent\")\n",
    "atc_list = []\n",
    "\n",
    "for link in BeautifulSoup(str(base), \"html.parser\").findAll(\"a\"):\n",
    "    if 'href' in link.attrs:\n",
    "        if link['href'].startswith('/wiki/') and len(link['href'].split('/')) == 3:\n",
    "#             print(link['href'])\n",
    "            atc_list.append(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-309037cc3484>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-309037cc3484>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    content = urllib2.urlopen(LANG['en'] + )\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "content = urllib2.urlopen(LANG['en'] + )\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "html = content.read()\n",
    "print html\n",
    "if html:\n",
    "    print unwiki(unhtml(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'simple'\n",
    "wiki = Wikipedia(lang)\n",
    "\n",
    "try:\n",
    "    raw = wiki.article('Uruguay')\n",
    "except:\n",
    "    raw = None\n",
    "\n",
    "if raw:\n",
    "    wiki2plain = Wiki2Plain(raw)\n",
    "    content = wiki2plain.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
